{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What is Transfer Learning ?\n",
    "- What are embeddings ?\n",
    "- How are Word2Vec vectors learned ?\n",
    "- What types of relationships do Word2Vec vectors capture ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Transfer Learning:**  is the concept of transfering the knowledge by using past model and training it to new dataset. When we have a small dataset but very diffcult problem wherein similar problem was earlier solved with a large dataset of similar distribtion, we can levergae the model learnt on the large dataset to train over the newer dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Embeddings:** are those feature representations that are extracted from pre trained networks\n",
    "- Are dense continous vector representation from neural networks that contain pertinent information & exhibit semantic relationships\n",
    "- Can be learned in supervised or unsupervised manner\n",
    "- Often useful as unsupervised data representation scheme (like PCA)\n",
    "- Sometimes also are called as 'thought vectors', 'x vectors', 'feature vectors'..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "import numpy as np\n",
    "import numpy\n",
    "from tqdm import tqdm\n",
    "import gensim\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Word2Vec Vectors [from](https://drive.google.com/uc?id=0B7XkCwpI5KDYNlNUTTlSS21pQmM&export=download)\n",
    "\n",
    "### Download analogies data [from](http://download.tensorflow.org/data/questions-words.txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](https://www.tensorflow.org/images/linear-relationships.png \"title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.KeyedVectors.load_word2vec_format('data/GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "king_vector = model['king']\n",
    "print(len(king_vector))\n",
    "print(king_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analogy_vector = model['king'] - model['man'] + model['queen']\n",
    "print(analogy_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example analogy task like king - man + woman = queen\n",
    "answer = model.most_similar(positive=['woman', 'king'], negative=['man'])\n",
    "print(\"king - man + woman = {}\".format(answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analogy_words = [line.rstrip('\\n').split(' ') for line in open('data/questions-words.txt')]\n",
    "analogy_words = [words for words in analogy_words if len(words) == 4]\n",
    "np.random.seed(0)\n",
    "analogy_words = random.sample(analogy_words, 100)\n",
    "X = [words[:3] for words in analogy_words]\n",
    "y = [words[3] for words in analogy_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X[0], y[0])\n",
    "print(X[10], y[10])\n",
    "print(X[50], y[50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "is_correct_list = []\n",
    "top_5_predictions_list = []\n",
    "for i in tqdm(range(len(X))):\n",
    "    components = X[i]\n",
    "    answer = y[i]\n",
    "    predictions = model.most_similar(positive=[components[1], components[2]], negative=[components[0]])\n",
    "    top_5_predictions = [p[0].lower() for p in sorted(predictions, key=lambda x : -x[1])[:10]]\n",
    "    top_5_predictions_list.append(top_5_predictions)\n",
    "    is_in_top_5 = 1.0 if answer.lower() in top_5_predictions else 0.0\n",
    "    is_correct_list.append(is_in_top_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    components = X[i]\n",
    "    answer = y[i]\n",
    "    top5 = top_5_predictions_list[i]\n",
    "    correct = is_correct_list[i]\n",
    "    print(\"Components: {}, Answer: {} Top5: {} is_correct: {}\".format(components, answer, top5, correct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy in Analogy Task is\", np.mean(is_correct_list))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
