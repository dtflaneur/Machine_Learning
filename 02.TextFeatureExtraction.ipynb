{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Extracting Features from Text\n",
    "##### Using Bag of Words, TF-IDF Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- First we Tokenise the document into individual words\n",
    "- Then represent each word by a number\n",
    "- Now the document can be expressed in form of a tensor/matrix\n",
    "- How do we assign these numbers to each word ? is there a method to do it ?\n",
    "- The numbers used to represent each word are called **Word Embeddings** & there are three ways in which embeddings can be assigned\n",
    "  - **One-hot Encoded Embeddings**\n",
    "  - **Frequency-based Embeddings**\n",
    "  - **Prediction-based Embeddings** - deep learning frameworks use this technique where the numerical representation of text capture meanings and semantic relationships\n",
    "\n",
    "**One-hot Encoding Embeddings**\n",
    "- **Disadvantages:**\n",
    "  - If we have large vocabulary then our feature vectors will be enormous (for ex if we have 10000 words in the corpus then every feature vector will be 10000 elements long)\n",
    "  - Does not capture the order of the occurence of the words hence the contextual information is lost \n",
    "  - Loss of frequency information i.e. we don't capture how many times the word appeared in the document\n",
    "  - One-hot encoding does NOT capture any semantic information or relationship between words\n",
    "\n",
    "**Frequency-based Embeddings**\n",
    " - Preserves the frequency of the words\n",
    " - Are of two types \n",
    "  - **Count Vectors:** \n",
    "        - counts how many times a word appeared in the particular document, thus it captures the frequency each word\n",
    "        - **Disadvantages:**\n",
    "        - if we have large vocabulary in the corpus we get enormous feature vectors for each document, to mitigate this we can\n",
    "        - Choose only the top N words based on frequency to counter this \n",
    "        - Hash words into buckets to have a fixed vocab size\n",
    "      - Does not capture the order of the occurence of the words hence the contextual information is lost \n",
    "      - Does not capture any semantic information or relationship between words\n",
    "  - **TF-IDF (Term frequency-Inverse document frequency):** \n",
    "    - It is a scoring algorithm which tracks how important the word is to the document & to the corpus as a whole\n",
    "    - It captures the word frequency in a document as well as in the entire corpus\n",
    "    - Every word is give a score by calculation xi = tf(wi) * idf(wi) (where tf is the word frequency in the word & idf is the inverse term frequency of that word in the whole document)\n",
    "    - tf will up weight the word which have higher frequency in the document idf will down weight the words which have high frequency in the corpus i.e. the common words (stop words)\n",
    "    - Thus tf-idf score for a word will be high if the word has high frequency in the document & less frequency in the corpus\n",
    "    - **Advantages:**\n",
    "      - Each feature vector is not as big as the whole corpus\n",
    "      - Frequency & relevance of each word is captured\n",
    "    - **Disadvantages:**\n",
    "      - Context is still not captured"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will look at these 3 vectorizers in sklearn\n",
    "- **CountVectorizer** - creates basic frequency based representation of words\n",
    "- **TfidfVectorizer**\n",
    "- **HashingVectorizer**\n",
    "\n",
    "Each vectorizer has a fit method to which we pass a corpus of documents, this method will generate unique IDs for all words in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define a corpus of 4 documents with some repeated values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = ['This is the first document.',\n",
    "          'This is the second document.', \n",
    "          'Third document. Document number three', \n",
    "          'Number four. To repeat, number four']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use CountVectorizer to convert a collection of text documents to a \"bag of words\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4x12 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 18 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "bag_of_words = vectorizer.fit_transform(corpus)\n",
    "bag_of_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View what the \"bag\" looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t1\n",
      "  (0, 1)\t1\n",
      "  (0, 7)\t1\n",
      "  (0, 3)\t1\n",
      "  (0, 9)\t1\n",
      "  (1, 6)\t1\n",
      "  (1, 0)\t1\n",
      "  (1, 7)\t1\n",
      "  (1, 3)\t1\n",
      "  (1, 9)\t1\n",
      "  (2, 10)\t1\n",
      "  (2, 4)\t1\n",
      "  (2, 8)\t1\n",
      "  (2, 0)\t2\n",
      "  (3, 5)\t1\n",
      "  (3, 11)\t1\n",
      "  (3, 2)\t2\n",
      "  (3, 4)\t2\n"
     ]
    }
   ],
   "source": [
    "print(bag_of_words) # the below format is (documentID, wordID) frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the value to which a word is mapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.vocabulary_.get('document') # will give the ID of the word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'this': 9,\n",
       " 'is': 3,\n",
       " 'the': 7,\n",
       " 'first': 1,\n",
       " 'document': 0,\n",
       " 'second': 6,\n",
       " 'third': 8,\n",
       " 'number': 4,\n",
       " 'three': 10,\n",
       " 'four': 2,\n",
       " 'to': 11,\n",
       " 'repeat': 5}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.vocabulary_ # typically most frequent words are assigned lower value ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.23.4\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "print(pd.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>first</th>\n",
       "      <th>four</th>\n",
       "      <th>is</th>\n",
       "      <th>number</th>\n",
       "      <th>repeat</th>\n",
       "      <th>second</th>\n",
       "      <th>the</th>\n",
       "      <th>third</th>\n",
       "      <th>this</th>\n",
       "      <th>three</th>\n",
       "      <th>to</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   document  first  four  is  number  repeat  second  the  third  this  three  \\\n",
       "0         1      1     0   1       0       0       0    1      0     1      0   \n",
       "1         1      0     0   1       0       0       1    1      0     1      0   \n",
       "2         2      0     0   0       1       0       0    0      1     0      1   \n",
       "3         0      0     2   0       2       1       0    0      0     0      0   \n",
       "\n",
       "   to  \n",
       "0   0  \n",
       "1   0  \n",
       "2   0  \n",
       "3   1  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(bag_of_words.toarray(), columns=vectorizer.get_feature_names())\n",
    "# below the rows are the individual documents & the column value are the corresponding word frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extend bag of words with TF-IDF weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 9)\t0.43584673255\n",
      "  (0, 3)\t0.43584673255\n",
      "  (0, 7)\t0.43584673255\n",
      "  (0, 1)\t0.552816315109\n",
      "  (0, 0)\t0.352855492979\n",
      "  (1, 9)\t0.43584673255\n",
      "  (1, 3)\t0.43584673255\n",
      "  (1, 7)\t0.43584673255\n",
      "  (1, 0)\t0.352855492979\n",
      "  (1, 6)\t0.552816315109\n",
      "  (2, 0)\t0.619139506794\n",
      "  (2, 8)\t0.485000839571\n",
      "  (2, 4)\t0.382380232698\n",
      "  (2, 10)\t0.485000839571\n",
      "  (3, 4)\t0.541279948942\n",
      "  (3, 2)\t0.686544981228\n",
      "  (3, 11)\t0.343272490614\n",
      "  (3, 5)\t0.343272490614\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "bag_of_words = vectorizer.fit_transform(corpus)\n",
    "\n",
    "print(bag_of_words) # the below format is (documentID, wordID) tf-idf score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.vocabulary_.get('document')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>first</th>\n",
       "      <th>four</th>\n",
       "      <th>is</th>\n",
       "      <th>number</th>\n",
       "      <th>repeat</th>\n",
       "      <th>second</th>\n",
       "      <th>the</th>\n",
       "      <th>third</th>\n",
       "      <th>this</th>\n",
       "      <th>three</th>\n",
       "      <th>to</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.352855</td>\n",
       "      <td>0.552816</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.435847</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.435847</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.435847</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.352855</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.435847</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.552816</td>\n",
       "      <td>0.435847</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.435847</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.619140</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.38238</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.485001</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.485001</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.686545</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.54128</td>\n",
       "      <td>0.343272</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.343272</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   document     first      four        is   number    repeat    second  \\\n",
       "0  0.352855  0.552816  0.000000  0.435847  0.00000  0.000000  0.000000   \n",
       "1  0.352855  0.000000  0.000000  0.435847  0.00000  0.000000  0.552816   \n",
       "2  0.619140  0.000000  0.000000  0.000000  0.38238  0.000000  0.000000   \n",
       "3  0.000000  0.000000  0.686545  0.000000  0.54128  0.343272  0.000000   \n",
       "\n",
       "        the     third      this     three        to  \n",
       "0  0.435847  0.000000  0.435847  0.000000  0.000000  \n",
       "1  0.435847  0.000000  0.435847  0.000000  0.000000  \n",
       "2  0.000000  0.485001  0.000000  0.485001  0.000000  \n",
       "3  0.000000  0.000000  0.000000  0.000000  0.343272  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(bag_of_words.toarray(), columns=vectorizer.get_feature_names())\n",
    "# below the rows are the individual documents & the column value are the corresponding tf-idf scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View all the words and their corresponding values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'document': 0,\n",
       " 'first': 1,\n",
       " 'four': 2,\n",
       " 'is': 3,\n",
       " 'number': 4,\n",
       " 'repeat': 5,\n",
       " 'second': 6,\n",
       " 'the': 7,\n",
       " 'third': 8,\n",
       " 'this': 9,\n",
       " 'three': 10,\n",
       " 'to': 11}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Hashing Vectorizer\n",
    "* One issue with CountVectorizer and TF-IDF Vectorizer is that the number of features can get very large if the vocabulary is very large\n",
    "* The whole vocabulary will be stored in memory, and this may end up taking a lot of space\n",
    "* With Hashing Vectorizer, one can limit the number of features, let's say to a number <b>n</b>\n",
    "* Each word will be hashed to one of the n values\n",
    "* There will collisions where different words will be hashed to the same value\n",
    "* In many instances, peformance does not really suffer in spite of the collisions\n",
    "* Thus we use HashingVectorizer when the feature set is very large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t-0.894427191\n",
      "  (0, 5)\t0.4472135955\n",
      "  (0, 6)\t0.0\n",
      "  (1, 0)\t-0.57735026919\n",
      "  (1, 3)\t0.57735026919\n",
      "  (1, 5)\t0.57735026919\n",
      "  (1, 6)\t0.0\n",
      "  (2, 0)\t-0.755928946018\n",
      "  (2, 3)\t0.377964473009\n",
      "  (2, 5)\t0.377964473009\n",
      "  (2, 7)\t0.377964473009\n",
      "  (3, 0)\t0.316227766017\n",
      "  (3, 3)\t0.316227766017\n",
      "  (3, 5)\t0.632455532034\n",
      "  (3, 7)\t0.632455532034\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "vectorizer = HashingVectorizer(n_features=8) # n_features represents the number of hash buckets to be used\n",
    "feature_vector = vectorizer.fit_transform(corpus)\n",
    "print(feature_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One disadvantage of HashingVectorizer is that there is no way to get back the words from the hashed value"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
