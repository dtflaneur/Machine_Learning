{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "#import seaborn as sns\n",
    "#import csv\n",
    "#from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download data [from](https://www.kaggle.com/c/avazu-ctr-prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame = pd.read_csv('data/train.csv', nrows=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encode the categorical variables\n",
    "pd.get_dummies(data_frame['site_category'], prefix='site_category').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform categorical data\n",
    "# go through all categorical variables & convert them to one-hot encode except fe which are not needed\n",
    "exclude_from_transformation = ['id', 'click', 'hour', 'device_ip', 'device_id']\n",
    "headers = data_frame.columns.tolist()\n",
    "for header in headers:\n",
    "    if header in exclude_from_transformation:\n",
    "        continue\n",
    "    one_hot = pd.get_dummies(data_frame[header], prefix=header)\n",
    "    data_frame = data_frame.drop(header, axis=1)\n",
    "    data_frame = data_frame.join(one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data_frame.drop(['click', 'id', 'hour', 'device_ip', 'device_id'], axis=1)\n",
    "Y = data_frame['click']\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=1/4., random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run standard Linear Regressions\n",
    "lm = LinearRegression()\n",
    "lm.fit(X_train, Y_train)\n",
    "\n",
    "Y_train_pred = lm.predict(X_train)\n",
    "Y_test_pred = lm.predict(X_test)\n",
    "\n",
    "train_mse = sklearn.metrics.mean_squared_error(Y_train, Y_train_pred)\n",
    "test_mse = sklearn.metrics.mean_squared_error(Y_test, Y_test_pred)\n",
    "print(\"Train MSE {}\".format(train_mse))\n",
    "print(\"Test MSE {}\".format(test_mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that the Test Mean Square Error is very large which is very bad and indicates that we have serious overfitting issue, this might happen because many times we have lot of categorical data having very sparse features & one of the feature would appear to be very important but in reality it is not that important thus regularisation becomes very important in such cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Regularised Ridge Regression\n",
    "lm_ridge = Ridge(alpha=0.5)\n",
    "lm_ridge.fit(X_train, Y_train)\n",
    "\n",
    "Y_train_pred = lm_ridge.predict(X_train)\n",
    "Y_test_pred = lm_ridge.predict(X_test)\n",
    "\n",
    "train_mse = sklearn.metrics.mean_squared_error(Y_train, Y_train_pred)\n",
    "test_mse = sklearn.metrics.mean_squared_error(Y_test, Y_test_pred)\n",
    "print(\"Ridge Regression Train MSE {}\".format(train_mse))\n",
    "print(\"Ridge Regression Test MSE {}\".format(test_mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we run Regularised Ridge Regression we see that the Test MSE is far much better than that of Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model for reuse\n",
    "from sklearn.externals import joblib\n",
    "joblib.dump(lm_ridge, 'models/ad_model.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression coefficient\n",
    "np.sum(lm.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge coefficient\n",
    "np.sum(lm_ridge.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that without regularisation the coefficient is huge & we face massive overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"resources/regularization.png\" alt=\"regularization\" width=\"70%\" height=\"70%\" border=\"1\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper Parameter tuning using GridSearch Cross Validation\n",
    "lm_ridge = Ridge()\n",
    "alphas = np.linspace(0.1, 5, 10)\n",
    "print(alphas)\n",
    "n_folds = 3\n",
    "\n",
    "clf = GridSearchCV(lm_ridge, [{'alpha': alphas}], cv=n_folds)\n",
    "clf.fit(X_train, Y_train)\n",
    "scores = clf.cv_results_['mean_test_score']\n",
    "plt.plot(alphas, scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    data_frame = pd.read_csv('ad_data/train.csv', nrows=2000)\n",
    "    exclude_from_transformation = ['id', 'click', 'hour', 'device_ip', 'device_id']\n",
    "    headers = data_frame.columns.tolist()\n",
    "    for header in headers:\n",
    "        if header in exclude_from_transformation:\n",
    "            continue\n",
    "        one_hot = pd.get_dummies(data_frame[header], prefix=header)\n",
    "        data_frame = data_frame.drop(header, axis=1)\n",
    "        data_frame = data_frame.join(one_hot)\n",
    "    X = data_frame.drop(['click', 'id', 'hour', 'device_ip', 'device_id'], axis=1)\n",
    "    Y = data_frame['click']\n",
    "    print(X.shape)\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=1/4., random_state=0)\n",
    "    return X_train, X_test, Y_train, Y_test\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = joblib.load(open('models/ad_model.pkl', 'rb'))\n",
    "ad_data, click_labels = X_test[:5], Y_test[:5]\n",
    "bids = [10, 20, 5, 12, 2]\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_ads(model, ads, bids):\n",
    "    ctr_preds = model.predict(ads)\n",
    "    rank_scores = np.array(ctr_preds*bids)\n",
    "    idx = np.argsort(-rank_scores)\n",
    "    return idx, ctr_preds, rank_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad_rankings, ctr_preds, rank_scores = rank_ads(model, ad_data, bids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad_rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctr_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctr_preds[ad_rankings]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
